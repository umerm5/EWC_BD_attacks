{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EWC_split_mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU_lXrtzO2h1",
        "colab_type": "code",
        "outputId": "85511af5-3f8a-4558-9fa4-eba66807af0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"Utility functions for benchmarking online learning\"\"\"\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist, cifar10, cifar100\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "import keras.backend as K\n",
        "\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Conv2D, AveragePooling2D, MaxPool2D, Flatten, InputLayer\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from util import computer_fisher, ewc_reg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.regularizers import Regularizer\n",
        "\n",
        "\n",
        "def computer_fisher(model, imgset, num_sample=30):\n",
        "    f_accum = []\n",
        "    for i in range(len(model.weights)):\n",
        "        f_accum.append(np.zeros(K.int_shape(model.weights[i])))\n",
        "    f_accum = np.array(f_accum)\n",
        "    for j in range(num_sample):\n",
        "        img_index = np.random.randint(imgset.shape[0])\n",
        "        for m in range(len(model.weights)):\n",
        "            grads = K.gradients(K.log(model.output), model.weights)[m]\n",
        "            result = K.function([model.input], [grads])\n",
        "            f_accum[m] += np.square(result([np.expand_dims(imgset[img_index], 0)])[0])\n",
        "    f_accum /= num_sample\n",
        "    return f_accum\n",
        "\n",
        "\n",
        "class ewc_reg(Regularizer):\n",
        "    def __init__(self, fisher, prior_weights, Lambda=0.99):\n",
        "        self.fisher = fisher\n",
        "        self.prior_weights = prior_weights\n",
        "        self.Lambda = Lambda\n",
        "\n",
        "    def __call__(self, x):\n",
        "        regularization = 0.\n",
        "        regularization += Lambda * K.sum(self.fisher * K.square(x - self.prior_weights))\n",
        "        return regularization\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'Lambda': float(Lambda)}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_dataset_by_labels(X, y, task_labels, nb_classes=None, multihead=False):\n",
        "    \"\"\"Split dataset by labels.\n",
        "\n",
        "    Args:\n",
        "        X: data\n",
        "        y: labels\n",
        "        task_labels: list of list of labels, one for each dataset\n",
        "        nb_classes: number of classes (used to convert to one-hot)\n",
        "    Returns:\n",
        "        List of (X, y) tuples representing each dataset\n",
        "    \"\"\"\n",
        "    if nb_classes is None:\n",
        "        nb_classes = len(np.unique(y))\n",
        "    datasets = []\n",
        "    for labels in task_labels:\n",
        "        idx = np.in1d(y, labels)\n",
        "        if multihead:\n",
        "            label_map = np.arange(nb_classes)\n",
        "            label_map[labels] = np.arange(len(labels))\n",
        "            data = X[idx], np_utils.to_categorical(label_map[y[idx]], len(labels))\n",
        "        else:\n",
        "            data = X[idx], np_utils.to_categorical(y[idx], nb_classes)\n",
        "        datasets.append(data)\n",
        "    return datasets\n",
        "\n",
        "\n",
        "def load_mnist(split='train'):\n",
        "  \n",
        "    # input image dimensions\n",
        "    img_rows, img_cols = 28, 28\n",
        "    \n",
        "    \n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "#     X_train = X_train.reshape(-1, 784)\n",
        "#     X_test = X_test.reshape(-1, 784)\n",
        "    \n",
        "    if K.image_data_format() == 'channels_first':\n",
        "      X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "      X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
        "      input_shape = (1, img_rows, img_cols)\n",
        "    else:\n",
        "      X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "      X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "      input_shape = (img_rows, img_cols, 1)\n",
        "      \n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "\n",
        "    if split == 'train':\n",
        "        X, y = X_train, y_train\n",
        "    else:\n",
        "        X, y = X_test, y_test\n",
        "    nb_classes = 10\n",
        "    y = np_utils.to_categorical(y, nb_classes)\n",
        "    return X, y\n",
        "\n",
        "def construct_split_mnist(task_labels,  split='train', multihead=False):\n",
        "    \"\"\"Split MNIST dataset by labels.\n",
        "\n",
        "        Args:\n",
        "                task_labels: list of list of labels, one for each dataset\n",
        "                split: whether to use train or testing data\n",
        "\n",
        "        Returns:\n",
        "            List of (X, y) tuples representing each dataset\n",
        "    \"\"\"\n",
        "    # Load MNIST data and normalize\n",
        "    nb_classes = 10\n",
        "    # input image dimensions\n",
        "    img_rows, img_cols = 28, 28\n",
        "    \n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    \n",
        "#     X_train = X_train.reshape(-1, 784)\n",
        "#     X_test = X_test.reshape(-1, 784)\n",
        "    \n",
        "    \n",
        "    if K.image_data_format() == 'channels_first':\n",
        "      X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "      X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
        "      input_shape = (1, img_rows, img_cols)\n",
        "    else:\n",
        "      X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "      X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "      input_shape = (img_rows, img_cols, 1)\n",
        "      \n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "    \n",
        "#     # convert class vectors to binary class matrices\n",
        "#     y_train = keras.utils.to_categorical(y_train, nb_classes)\n",
        "#     y_test = keras.utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    if split == 'train':\n",
        "        X, y = X_train, y_train\n",
        "    else:\n",
        "        X, y = X_test, y_test\n",
        "\n",
        "    return split_dataset_by_labels(X, y, task_labels, nb_classes, multihead)\n",
        "  \n",
        "  \n",
        "class TestCallback(keras.callbacks.Callback):\n",
        "  def __init__(self, test_data):\n",
        "      self.test_data = test_data\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "      x, y = self.test_data\n",
        "      loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "      if(acc > 0.95):\n",
        "        Lambda = 1000 * S_lam\n",
        "        print(\"lam=\",Lambda)\n",
        "      elif(acc > 0.90):\n",
        "        Lambda = 800 * S_lam\n",
        "        print(\"lam=\",Lambda)\n",
        "      elif(acc > 0.85):\n",
        "        Lambda = 700 * S_lam\n",
        "        print(\"lam=\",Lambda)\n",
        "      elif(acc > 0.80):\n",
        "        Lambda = 600 * S_lam\n",
        "        print(\"lam=\",Lambda)\n",
        "      elif(acc > 0.75):\n",
        "        Lambda = 300 * S_lam\n",
        "        print(\"lam=\",Lambda)\n",
        "      elif(acc > 0.70):\n",
        "        Lambda = 200 * S_lam\n",
        "        print(\"lam=\",Lambda)\n",
        "      elif(acc > 0.65):\n",
        "        Lambda = 100 * S_lam\n",
        "        print(\"lam=\",Lambda)\n",
        "      #print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
        "      \n",
        "\n",
        "np.random.seed(104)\n",
        "Batch_size = 65536\n",
        "Epochs = 100\n",
        "\n",
        "global S_lam \n",
        "global Lambda\n",
        "S_lam = 0.1\n",
        "Lambda = S_lam\n",
        "\n",
        "task_labels = [[0,1], [2,3]]#, [4,5], [6,7], [8,9]]\n",
        "#task_labels = [[0,1], [2,3], [4,5], [6,7], [8,9]]\n",
        "# task_labels = [[0,1,2,3,4], [5,6,7,8,9]]\n",
        "n_tasks = len(task_labels)\n",
        "training_datasets =  construct_split_mnist(task_labels, split='train')\n",
        "validation_datasets = construct_split_mnist(task_labels, split='test')\n",
        "\n",
        "print(validation_datasets[0][1].shape)\n",
        "#\n",
        "# ####Display three Tasks Dataset images\n",
        "# plt.figure()\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.imshow(training_datasets[0][0][0], cmap='gray')\n",
        "# plt.title('Task A')\n",
        "# plt.axis('off')\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.imshow(training_datasets[1][0][0], cmap='gray')\n",
        "# plt.title('Task B')\n",
        "# plt.axis('off')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "##### Task A training and save the prior weights for the next Task\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(28,28,1)))\n",
        "model.add(Conv2D(8, (5, 5), padding=\"same\", activation=\"relu\"))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(16, (5, 5), padding=\"same\", activation=\"relu\"))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.summary()\n",
        "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_datasets[0][0], training_datasets[0][1], Batch_size, Epochs, validation_data=(validation_datasets[0][0], validation_datasets[0][1]))\n",
        "model.save('MNISTA.h5')\n",
        "\n",
        "\n",
        "\n",
        "##### Compute the Fisher Information for each parameter in Task A\n",
        "print('Processing Fisher Information...')\n",
        "I = computer_fisher(model, training_datasets[0][0])\n",
        "print('Processing Finish!')\n",
        "\n",
        "##### Task B EWC training\n",
        "model_ewcB = Sequential()\n",
        "model_ewcB.add(InputLayer(input_shape=(28,28,1)))\n",
        "print(\"lam_check=\",Lambda)\n",
        "model_ewcB.add(Conv2D(8, (5, 5), padding=\"same\", activation=\"relu\",kernel_regularizer=ewc_reg(I[0], model.weights[0]),\n",
        "                      bias_regularizer=ewc_reg(I[1], model.weights[1])))\n",
        "lam = ewc_reg.get_config(ewc_reg(I[1], model.weights[1]))\n",
        "print(\"lam_check=\",lam)\n",
        "model_ewcB.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "model_ewcB.add(Conv2D(16, (5, 5), padding=\"same\", activation=\"relu\",kernel_regularizer=ewc_reg(I[2], model.weights[2]),\n",
        "                      bias_regularizer=ewc_reg(I[3], model.weights[3])))\n",
        "model_ewcB.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "# model_ewcB.add(Dense(10, activation='relu', input_dim=784, kernel_regularizer=ewc_reg(I[0], model.weights[0]),\n",
        "#                  bias_regularizer=ewc_reg(I[1], model.weights[1])))\n",
        "model_ewcB.add(Flatten())\n",
        "model_ewcB.add(Dense(10, activation='softmax', kernel_regularizer=ewc_reg(I[4], model.weights[4]),\n",
        "                 bias_regularizer=ewc_reg(I[5], model.weights[5])))\n",
        "model_ewcB.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model_ewcB.load_weights('MNISTA.h5')\n",
        "model_ewcB.fit(training_datasets[1][0], training_datasets[1][1],Batch_size, Epochs, validation_data=(validation_datasets[1][0],validation_datasets[1][1]),\n",
        "              callbacks=[TestCallback((validation_datasets[1][0],validation_datasets[1][1]))])\n",
        "\n",
        "# # Task B no penalty training\n",
        "# model_NoP_B = Sequential()\n",
        "# model_NoP_B.add(Dense(10, activation='relu', input_dim=784))\n",
        "# model_NoP_B.add(Dense(10, activation='softmax'))\n",
        "# model_NoP_B.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
        "# model_NoP_B.load_weights('MNISTA.h5')\n",
        "# model_NoP_B.fit(training_datasets[1][0], training_datasets[1][1], 100, 10, validation_data=(validation_datasets[1][0],validation_datasets[1][1]))\n",
        "\n",
        "# Current Task Performance\n",
        "B_EWC = 100 * model_ewcB.evaluate(validation_datasets[1][0],validation_datasets[1][1], verbose=0)[1]\n",
        "# B_No_P = 100 * model_NoP_B.evaluate(validation_datasets[1][0],validation_datasets[1][1], verbose=0)[1]\n",
        "# Previous Task Performance\n",
        "A_EWC = 100 * model_ewcB.evaluate(validation_datasets[0][0],validation_datasets[0][1], verbose=0)[1]\n",
        "# A_No_P = 100 * model_NoP_B.evaluate(validation_datasets[0][0],validation_datasets[0][1], verbose=0)[1]\n",
        "\n",
        "print(\"Task A Original Accuracy: %.2f%%\" % (100 * model.evaluate(validation_datasets[0][0], validation_datasets[0][1])[1]))\n",
        "print(\"Task B EWC method penalty Accuracy: %.2f%%\" % B_EWC)\n",
        "# print(\"Task B SGD method Accuracy: %.2f%%\" % B_No_P)\n",
        "print(\"Task A EWC method penalty Accuracy: %.2f%%\" % A_EWC)\n",
        "# print(\"Task A SGD method Accuracy: %.2f%%\" % A_No_P)\n",
        "\n",
        "x = 0\n",
        "total_width, n = 0.1, 2\n",
        "width = total_width / n\n",
        "x = x - (total_width - width) / 2\n",
        "plt.style.use('ggplot')\n",
        "plt.bar(x, B_EWC, width=width, label='EWC Task B', hatch='w/', ec='w')\n",
        "# plt.bar(x + width, B_No_P, width=width, label='SGD Task B', hatch='w/', ec='w')\n",
        "plt.bar(x + 3.5 * width, A_EWC, width=width, label='EWC Task A', hatch='w/', ec='w')\n",
        "# plt.bar(x + 4.5 * width, A_No_P, width=width, label='SGD Task A', hatch='w/', ec='w')\n",
        "plt.legend(facecolor='white')\n",
        "plt.xticks(np.array([0., 3.5 * width]), ('Current', 'Previous'))\n",
        "plt.title('EWC method vs SGD method on \\n Current task and Previous task')\n",
        "plt.xlim(-0.15, 0.35)\n",
        "plt.ylim(0., 105.)\n",
        "plt.show()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2115, 10)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_42 (Conv2D)           (None, 28, 28, 8)         208       \n",
            "_________________________________________________________________\n",
            "average_pooling2d_39 (Averag (None, 14, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 14, 14, 16)        3216      \n",
            "_________________________________________________________________\n",
            "average_pooling2d_40 (Averag (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_20 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 11,274\n",
            "Trainable params: 11,274\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 12665 samples, validate on 2115 samples\n",
            "Epoch 1/100\n",
            "12665/12665 [==============================] - 16s 1ms/step - loss: 2.2471 - acc: 0.2154 - val_loss: 2.2096 - val_acc: 0.4222\n",
            "Epoch 2/100\n",
            "12665/12665 [==============================] - 7s 544us/step - loss: 2.2093 - acc: 0.4168 - val_loss: 2.1621 - val_acc: 0.6170\n",
            "Epoch 3/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 2.1616 - acc: 0.6250 - val_loss: 2.1058 - val_acc: 0.9031\n",
            "Epoch 4/100\n",
            "12665/12665 [==============================] - 7s 529us/step - loss: 2.1051 - acc: 0.8994 - val_loss: 2.0399 - val_acc: 0.9258\n",
            "Epoch 5/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 2.0389 - acc: 0.9215 - val_loss: 1.9631 - val_acc: 0.9239\n",
            "Epoch 6/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 1.9618 - acc: 0.9161 - val_loss: 1.8741 - val_acc: 0.9111\n",
            "Epoch 7/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 1.8724 - acc: 0.9034 - val_loss: 1.7719 - val_acc: 0.8936\n",
            "Epoch 8/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 1.7699 - acc: 0.8869 - val_loss: 1.6564 - val_acc: 0.8652\n",
            "Epoch 9/100\n",
            "12665/12665 [==============================] - 7s 522us/step - loss: 1.6541 - acc: 0.8676 - val_loss: 1.5287 - val_acc: 0.8430\n",
            "Epoch 10/100\n",
            "12665/12665 [==============================] - 7s 532us/step - loss: 1.5261 - acc: 0.8450 - val_loss: 1.3916 - val_acc: 0.8137\n",
            "Epoch 11/100\n",
            "12665/12665 [==============================] - 7s 530us/step - loss: 1.3887 - acc: 0.8198 - val_loss: 1.2495 - val_acc: 0.7835\n",
            "Epoch 12/100\n",
            "12665/12665 [==============================] - 7s 526us/step - loss: 1.2466 - acc: 0.7893 - val_loss: 1.1088 - val_acc: 0.7461\n",
            "Epoch 13/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 1.1060 - acc: 0.7541 - val_loss: 0.9761 - val_acc: 0.7012\n",
            "Epoch 14/100\n",
            "12665/12665 [==============================] - 7s 530us/step - loss: 0.9736 - acc: 0.7105 - val_loss: 0.8578 - val_acc: 0.6487\n",
            "Epoch 15/100\n",
            "12665/12665 [==============================] - 7s 530us/step - loss: 0.8558 - acc: 0.6598 - val_loss: 0.7581 - val_acc: 0.6052\n",
            "Epoch 16/100\n",
            "12665/12665 [==============================] - 7s 535us/step - loss: 0.7565 - acc: 0.6138 - val_loss: 0.6774 - val_acc: 0.5882\n",
            "Epoch 17/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 0.6762 - acc: 0.5950 - val_loss: 0.6130 - val_acc: 0.6024\n",
            "Epoch 18/100\n",
            "12665/12665 [==============================] - 7s 524us/step - loss: 0.6121 - acc: 0.6145 - val_loss: 0.5599 - val_acc: 0.6681\n",
            "Epoch 19/100\n",
            "12665/12665 [==============================] - 7s 526us/step - loss: 0.5596 - acc: 0.6782 - val_loss: 0.5134 - val_acc: 0.7626\n",
            "Epoch 20/100\n",
            "12665/12665 [==============================] - 7s 529us/step - loss: 0.5138 - acc: 0.7686 - val_loss: 0.4702 - val_acc: 0.8596\n",
            "Epoch 21/100\n",
            "12665/12665 [==============================] - 7s 524us/step - loss: 0.4716 - acc: 0.8621 - val_loss: 0.4295 - val_acc: 0.9489\n",
            "Epoch 22/100\n",
            "12665/12665 [==============================] - 7s 526us/step - loss: 0.4321 - acc: 0.9386 - val_loss: 0.3920 - val_acc: 0.9797\n",
            "Epoch 23/100\n",
            "12665/12665 [==============================] - 7s 531us/step - loss: 0.3962 - acc: 0.9739 - val_loss: 0.3592 - val_acc: 0.9882\n",
            "Epoch 24/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.3651 - acc: 0.9821 - val_loss: 0.3315 - val_acc: 0.9853\n",
            "Epoch 25/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 0.3391 - acc: 0.9768 - val_loss: 0.3074 - val_acc: 0.9778\n",
            "Epoch 26/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.3167 - acc: 0.9690 - val_loss: 0.2846 - val_acc: 0.9735\n",
            "Epoch 27/100\n",
            "12665/12665 [==============================] - 7s 525us/step - loss: 0.2951 - acc: 0.9639 - val_loss: 0.2611 - val_acc: 0.9712\n",
            "Epoch 28/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.2723 - acc: 0.9630 - val_loss: 0.2370 - val_acc: 0.9749\n",
            "Epoch 29/100\n",
            "12665/12665 [==============================] - 7s 523us/step - loss: 0.2485 - acc: 0.9664 - val_loss: 0.2135 - val_acc: 0.9792\n",
            "Epoch 30/100\n",
            "12665/12665 [==============================] - 7s 529us/step - loss: 0.2249 - acc: 0.9715 - val_loss: 0.1923 - val_acc: 0.9835\n",
            "Epoch 31/100\n",
            "12665/12665 [==============================] - 7s 523us/step - loss: 0.2034 - acc: 0.9758 - val_loss: 0.1740 - val_acc: 0.9896\n",
            "Epoch 32/100\n",
            "12665/12665 [==============================] - 7s 530us/step - loss: 0.1849 - acc: 0.9803 - val_loss: 0.1587 - val_acc: 0.9910\n",
            "Epoch 33/100\n",
            "12665/12665 [==============================] - 7s 529us/step - loss: 0.1693 - acc: 0.9826 - val_loss: 0.1454 - val_acc: 0.9915\n",
            "Epoch 34/100\n",
            "12665/12665 [==============================] - 7s 529us/step - loss: 0.1559 - acc: 0.9843 - val_loss: 0.1334 - val_acc: 0.9943\n",
            "Epoch 35/100\n",
            "12665/12665 [==============================] - 7s 524us/step - loss: 0.1440 - acc: 0.9853 - val_loss: 0.1221 - val_acc: 0.9948\n",
            "Epoch 36/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.1329 - acc: 0.9861 - val_loss: 0.1114 - val_acc: 0.9948\n",
            "Epoch 37/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.1224 - acc: 0.9862 - val_loss: 0.1013 - val_acc: 0.9948\n",
            "Epoch 38/100\n",
            "12665/12665 [==============================] - 7s 529us/step - loss: 0.1127 - acc: 0.9862 - val_loss: 0.0920 - val_acc: 0.9943\n",
            "Epoch 39/100\n",
            "12665/12665 [==============================] - 7s 524us/step - loss: 0.1040 - acc: 0.9854 - val_loss: 0.0839 - val_acc: 0.9934\n",
            "Epoch 40/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 0.0964 - acc: 0.9851 - val_loss: 0.0769 - val_acc: 0.9924\n",
            "Epoch 41/100\n",
            "12665/12665 [==============================] - 7s 524us/step - loss: 0.0899 - acc: 0.9845 - val_loss: 0.0710 - val_acc: 0.9920\n",
            "Epoch 42/100\n",
            "12665/12665 [==============================] - 7s 533us/step - loss: 0.0845 - acc: 0.9842 - val_loss: 0.0658 - val_acc: 0.9915\n",
            "Epoch 43/100\n",
            "12665/12665 [==============================] - 7s 525us/step - loss: 0.0797 - acc: 0.9840 - val_loss: 0.0613 - val_acc: 0.9905\n",
            "Epoch 44/100\n",
            "12665/12665 [==============================] - 7s 529us/step - loss: 0.0756 - acc: 0.9838 - val_loss: 0.0573 - val_acc: 0.9905\n",
            "Epoch 45/100\n",
            "12665/12665 [==============================] - 7s 526us/step - loss: 0.0717 - acc: 0.9841 - val_loss: 0.0537 - val_acc: 0.9910\n",
            "Epoch 46/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.0681 - acc: 0.9847 - val_loss: 0.0503 - val_acc: 0.9929\n",
            "Epoch 47/100\n",
            "12665/12665 [==============================] - 7s 525us/step - loss: 0.0648 - acc: 0.9852 - val_loss: 0.0473 - val_acc: 0.9939\n",
            "Epoch 48/100\n",
            "12665/12665 [==============================] - 7s 530us/step - loss: 0.0616 - acc: 0.9858 - val_loss: 0.0446 - val_acc: 0.9948\n",
            "Epoch 49/100\n",
            "12665/12665 [==============================] - 7s 524us/step - loss: 0.0587 - acc: 0.9869 - val_loss: 0.0423 - val_acc: 0.9948\n",
            "Epoch 50/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 0.0562 - acc: 0.9878 - val_loss: 0.0402 - val_acc: 0.9948\n",
            "Epoch 51/100\n",
            "12665/12665 [==============================] - 7s 524us/step - loss: 0.0539 - acc: 0.9886 - val_loss: 0.0384 - val_acc: 0.9948\n",
            "Epoch 52/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.0518 - acc: 0.9889 - val_loss: 0.0368 - val_acc: 0.9953\n",
            "Epoch 53/100\n",
            "12665/12665 [==============================] - 7s 525us/step - loss: 0.0500 - acc: 0.9892 - val_loss: 0.0354 - val_acc: 0.9953\n",
            "Epoch 54/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.0484 - acc: 0.9893 - val_loss: 0.0340 - val_acc: 0.9953\n",
            "Epoch 55/100\n",
            "12665/12665 [==============================] - 7s 524us/step - loss: 0.0470 - acc: 0.9890 - val_loss: 0.0328 - val_acc: 0.9957\n",
            "Epoch 56/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 0.0456 - acc: 0.9894 - val_loss: 0.0315 - val_acc: 0.9957\n",
            "Epoch 57/100\n",
            "12665/12665 [==============================] - 7s 523us/step - loss: 0.0443 - acc: 0.9894 - val_loss: 0.0303 - val_acc: 0.9957\n",
            "Epoch 58/100\n",
            "12665/12665 [==============================] - 7s 524us/step - loss: 0.0430 - acc: 0.9897 - val_loss: 0.0292 - val_acc: 0.9962\n",
            "Epoch 59/100\n",
            "12665/12665 [==============================] - 7s 524us/step - loss: 0.0419 - acc: 0.9901 - val_loss: 0.0281 - val_acc: 0.9962\n",
            "Epoch 60/100\n",
            "12665/12665 [==============================] - 7s 529us/step - loss: 0.0407 - acc: 0.9903 - val_loss: 0.0270 - val_acc: 0.9962\n",
            "Epoch 61/100\n",
            "12665/12665 [==============================] - 7s 526us/step - loss: 0.0397 - acc: 0.9907 - val_loss: 0.0260 - val_acc: 0.9962\n",
            "Epoch 62/100\n",
            "12665/12665 [==============================] - 7s 532us/step - loss: 0.0387 - acc: 0.9909 - val_loss: 0.0251 - val_acc: 0.9962\n",
            "Epoch 63/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 0.0378 - acc: 0.9910 - val_loss: 0.0242 - val_acc: 0.9962\n",
            "Epoch 64/100\n",
            "12665/12665 [==============================] - 7s 529us/step - loss: 0.0369 - acc: 0.9909 - val_loss: 0.0234 - val_acc: 0.9962\n",
            "Epoch 65/100\n",
            "12665/12665 [==============================] - 7s 523us/step - loss: 0.0362 - acc: 0.9910 - val_loss: 0.0227 - val_acc: 0.9962\n",
            "Epoch 66/100\n",
            "12665/12665 [==============================] - 7s 526us/step - loss: 0.0354 - acc: 0.9912 - val_loss: 0.0220 - val_acc: 0.9962\n",
            "Epoch 67/100\n",
            "12665/12665 [==============================] - 7s 526us/step - loss: 0.0347 - acc: 0.9914 - val_loss: 0.0214 - val_acc: 0.9962\n",
            "Epoch 68/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.0340 - acc: 0.9916 - val_loss: 0.0208 - val_acc: 0.9962\n",
            "Epoch 69/100\n",
            "12665/12665 [==============================] - 7s 523us/step - loss: 0.0334 - acc: 0.9916 - val_loss: 0.0203 - val_acc: 0.9962\n",
            "Epoch 70/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.0327 - acc: 0.9917 - val_loss: 0.0198 - val_acc: 0.9967\n",
            "Epoch 71/100\n",
            "12665/12665 [==============================] - 7s 522us/step - loss: 0.0321 - acc: 0.9917 - val_loss: 0.0193 - val_acc: 0.9967\n",
            "Epoch 72/100\n",
            "12665/12665 [==============================] - 7s 526us/step - loss: 0.0315 - acc: 0.9919 - val_loss: 0.0189 - val_acc: 0.9972\n",
            "Epoch 73/100\n",
            "12665/12665 [==============================] - 7s 524us/step - loss: 0.0309 - acc: 0.9919 - val_loss: 0.0185 - val_acc: 0.9967\n",
            "Epoch 74/100\n",
            "12665/12665 [==============================] - 7s 530us/step - loss: 0.0304 - acc: 0.9922 - val_loss: 0.0181 - val_acc: 0.9967\n",
            "Epoch 75/100\n",
            "12665/12665 [==============================] - 7s 524us/step - loss: 0.0298 - acc: 0.9925 - val_loss: 0.0177 - val_acc: 0.9967\n",
            "Epoch 76/100\n",
            "12665/12665 [==============================] - 7s 531us/step - loss: 0.0293 - acc: 0.9926 - val_loss: 0.0174 - val_acc: 0.9967\n",
            "Epoch 77/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 0.0288 - acc: 0.9928 - val_loss: 0.0171 - val_acc: 0.9972\n",
            "Epoch 78/100\n",
            "12665/12665 [==============================] - 7s 529us/step - loss: 0.0283 - acc: 0.9928 - val_loss: 0.0167 - val_acc: 0.9976\n",
            "Epoch 79/100\n",
            "12665/12665 [==============================] - 7s 525us/step - loss: 0.0279 - acc: 0.9928 - val_loss: 0.0164 - val_acc: 0.9976\n",
            "Epoch 80/100\n",
            "12665/12665 [==============================] - 7s 532us/step - loss: 0.0274 - acc: 0.9930 - val_loss: 0.0161 - val_acc: 0.9976\n",
            "Epoch 81/100\n",
            "12665/12665 [==============================] - 7s 523us/step - loss: 0.0270 - acc: 0.9931 - val_loss: 0.0158 - val_acc: 0.9976\n",
            "Epoch 82/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.0265 - acc: 0.9931 - val_loss: 0.0155 - val_acc: 0.9976\n",
            "Epoch 83/100\n",
            "12665/12665 [==============================] - 7s 523us/step - loss: 0.0261 - acc: 0.9931 - val_loss: 0.0152 - val_acc: 0.9976\n",
            "Epoch 84/100\n",
            "12665/12665 [==============================] - 7s 526us/step - loss: 0.0257 - acc: 0.9932 - val_loss: 0.0149 - val_acc: 0.9976\n",
            "Epoch 85/100\n",
            "12665/12665 [==============================] - 7s 525us/step - loss: 0.0253 - acc: 0.9934 - val_loss: 0.0146 - val_acc: 0.9976\n",
            "Epoch 86/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.0249 - acc: 0.9937 - val_loss: 0.0143 - val_acc: 0.9976\n",
            "Epoch 87/100\n",
            "12665/12665 [==============================] - 7s 526us/step - loss: 0.0246 - acc: 0.9938 - val_loss: 0.0140 - val_acc: 0.9976\n",
            "Epoch 88/100\n",
            "12665/12665 [==============================] - 7s 529us/step - loss: 0.0242 - acc: 0.9938 - val_loss: 0.0137 - val_acc: 0.9976\n",
            "Epoch 89/100\n",
            "12665/12665 [==============================] - 7s 524us/step - loss: 0.0238 - acc: 0.9939 - val_loss: 0.0135 - val_acc: 0.9976\n",
            "Epoch 90/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 0.0235 - acc: 0.9939 - val_loss: 0.0132 - val_acc: 0.9976\n",
            "Epoch 91/100\n",
            "12665/12665 [==============================] - 7s 525us/step - loss: 0.0231 - acc: 0.9939 - val_loss: 0.0130 - val_acc: 0.9976\n",
            "Epoch 92/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 0.0228 - acc: 0.9940 - val_loss: 0.0127 - val_acc: 0.9976\n",
            "Epoch 93/100\n",
            "12665/12665 [==============================] - 7s 525us/step - loss: 0.0225 - acc: 0.9941 - val_loss: 0.0125 - val_acc: 0.9976\n",
            "Epoch 94/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.0222 - acc: 0.9942 - val_loss: 0.0123 - val_acc: 0.9976\n",
            "Epoch 95/100\n",
            "12665/12665 [==============================] - 7s 526us/step - loss: 0.0218 - acc: 0.9942 - val_loss: 0.0121 - val_acc: 0.9976\n",
            "Epoch 96/100\n",
            "12665/12665 [==============================] - 7s 529us/step - loss: 0.0215 - acc: 0.9942 - val_loss: 0.0119 - val_acc: 0.9976\n",
            "Epoch 97/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 0.0212 - acc: 0.9943 - val_loss: 0.0117 - val_acc: 0.9976\n",
            "Epoch 98/100\n",
            "12665/12665 [==============================] - 7s 528us/step - loss: 0.0209 - acc: 0.9945 - val_loss: 0.0115 - val_acc: 0.9976\n",
            "Epoch 99/100\n",
            "12665/12665 [==============================] - 7s 527us/step - loss: 0.0206 - acc: 0.9946 - val_loss: 0.0113 - val_acc: 0.9976\n",
            "Epoch 100/100\n",
            "12665/12665 [==============================] - 7s 530us/step - loss: 0.0204 - acc: 0.9949 - val_loss: 0.0112 - val_acc: 0.9976\n",
            "Processing Fisher Information...\n",
            "Processing Finish!\n",
            "lam_check= 0.1\n",
            "lam_check= {'Lambda': 0.1}\n",
            "Train on 12089 samples, validate on 2042 samples\n",
            "Epoch 1/100\n",
            "12089/12089 [==============================] - 17s 1ms/step - loss: 16.1063 - acc: 0.0000e+00 - val_loss: 16.1060 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "12089/12089 [==============================] - 6s 532us/step - loss: 16.1070 - acc: 0.0000e+00 - val_loss: 16.0780 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "12089/12089 [==============================] - 6s 525us/step - loss: 16.0805 - acc: 0.0000e+00 - val_loss: 16.0538 - val_acc: 0.0000e+00\n",
            "Epoch 4/100\n",
            "12089/12089 [==============================] - 6s 535us/step - loss: 16.0556 - acc: 0.0000e+00 - val_loss: 15.9882 - val_acc: 0.0000e+00\n",
            "Epoch 5/100\n",
            "12089/12089 [==============================] - 6s 530us/step - loss: 15.9868 - acc: 0.0000e+00 - val_loss: 15.8437 - val_acc: 0.0000e+00\n",
            "Epoch 6/100\n",
            "12089/12089 [==============================] - 6s 529us/step - loss: 15.8396 - acc: 0.0000e+00 - val_loss: 15.5874 - val_acc: 0.0000e+00\n",
            "Epoch 7/100\n",
            "12089/12089 [==============================] - 6s 528us/step - loss: 15.5811 - acc: 0.0000e+00 - val_loss: 15.1801 - val_acc: 0.0000e+00\n",
            "Epoch 8/100\n",
            "12089/12089 [==============================] - 6s 533us/step - loss: 15.1646 - acc: 0.0000e+00 - val_loss: 14.5789 - val_acc: 0.0000e+00\n",
            "Epoch 9/100\n",
            "12089/12089 [==============================] - 6s 528us/step - loss: 14.5429 - acc: 0.0000e+00 - val_loss: 13.7856 - val_acc: 0.0000e+00\n",
            "Epoch 10/100\n",
            "12089/12089 [==============================] - 6s 532us/step - loss: 13.7158 - acc: 0.0000e+00 - val_loss: 12.8056 - val_acc: 0.0000e+00\n",
            "Epoch 11/100\n",
            "12089/12089 [==============================] - 6s 529us/step - loss: 12.7342 - acc: 0.0000e+00 - val_loss: 11.7540 - val_acc: 0.0000e+00\n",
            "Epoch 12/100\n",
            "12089/12089 [==============================] - 7s 540us/step - loss: 11.6880 - acc: 0.0000e+00 - val_loss: 10.7355 - val_acc: 0.0000e+00\n",
            "Epoch 13/100\n",
            "12089/12089 [==============================] - 6s 531us/step - loss: 10.6736 - acc: 0.0000e+00 - val_loss: 9.8124 - val_acc: 0.0000e+00\n",
            "Epoch 14/100\n",
            "12089/12089 [==============================] - 6s 536us/step - loss: 9.7569 - acc: 0.0000e+00 - val_loss: 9.0105 - val_acc: 0.0000e+00\n",
            "Epoch 15/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-126838af3454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0mmodel_ewcB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MNISTA.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m model_ewcB.fit(training_datasets[1][0], training_datasets[1][1],Batch_size, Epochs, validation_data=(validation_datasets[1][0],validation_datasets[1][1]),\n\u001b[0;32m--> 272\u001b[0;31m               callbacks=[TestCallback((validation_datasets[1][0],validation_datasets[1][1]))])\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;31m# # Task B no penalty training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}